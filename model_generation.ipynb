{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3711e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import sklearn\n",
    "import pickle\n",
    "import os\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV, cross_val_score,TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8d139c-97b1-4830-896f-ec10d706451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_wd():\n",
    "    # Get the GitHub Actions workspace directory\n",
    "    workspace = os.getenv('GITHUB_WORKSPACE', '.')\n",
    "    \n",
    "    # Set the working directory to the folder where the data resides\n",
    "    cleaned_data = os.path.join(workspace, 'cleaned data')\n",
    "    website_code = os.path.join(workspace, 'Website code')\n",
    "    return cleaned_data,website_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba621a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load the dataset\n",
    "    os.chdir(cleaned_data)\n",
    "    match_results = pd.read_csv('afl_match_results_cleaned.csv')\n",
    "    os.chdir(website_code)\n",
    "    \n",
    "    # Define the features and the target variable\n",
    "    weather_dummies = pd.get_dummies(match_results['weather.weatherType'])\n",
    "    X = match_results.drop(columns=['match.homeTeam.name', 'match.awayTeam.name','venue.name','Margin','Result','weather.weatherType']).astype('float64')  # Drop irrelevant columns\n",
    "    X = pd.concat([X, weather_dummies], axis=1)\n",
    "    y = match_results['Result']  # BW, LW, D, LL, BL\n",
    "    \n",
    "    # Assuming 'weather_columns' is a list of your dummy weather variables\n",
    "    weather_columns = weather_dummies.columns  # Replace with actual weather columns\n",
    "    discrete_columns = ['Home.Team.Venue.Win.Streak', 'Away.Team.Venue.Win.Streak','Home.Win.Streak'] \n",
    "    continuous_columns = [col for col in X.columns if col not in weather_columns and col not in discrete_columns]\n",
    "    \n",
    "    # ColumnTransformer to apply StandardScaler only to continuous features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), continuous_columns),\n",
    "            ('disc', MinMaxScaler(), discrete_columns),\n",
    "            ('weather', 'passthrough', weather_columns)  # Weather columns are passed through unchanged\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Initialize LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit and transform the target variable\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    \n",
    "    cutoff_index = int(0.8 * len(match_results))\n",
    "    \n",
    "    y_train = encoder.fit_transform(y)    \n",
    "    \n",
    "    # Standardize the features\n",
    "    X_train = preprocessor.fit_transform(X)\n",
    "\n",
    "    return encoder,preprocessor,X_train,y_train,X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07026fcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run at beginning of season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e2964-4316-4b9d-a07b-2523473c96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TimeSeriesSplit for time-aware cross-validation\n",
    "# tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# # Function to create the Keras model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, neurons=64, learn_rate=0.01):\n",
    "#     model = Sequential([\n",
    "#         Dense(neurons, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.001), input_dim=X_train.shape[1]),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(dropout_rate),\n",
    "#         Dense(32, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(dropout_rate),\n",
    "#         Dense(16, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(dropout_rate),\n",
    "#         Dense(5, kernel_initializer='he_uniform', activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learn_rate, name=optimizer), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Example: Cross-validate Neural Network (Keras) base model\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the grid of hyperparameters to search\n",
    "# nn_param_grid = {\n",
    "#     'batch_size': [32, 64, 128],\n",
    "#     'epochs': [50],\n",
    "#     'optimizer': ['adam', 'sgd'],\n",
    "#     'dropout_rate': [0.2, 0.3, 0.5],\n",
    "#     'neurons': [32, 64, 128],\n",
    "#     'learn_rate': [0.001, 0.01, 0.1]\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search_nn = GridSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_grid=nn_param_grid,\n",
    "#     cv=tscv,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Fit the model using GridSearchCV\n",
    "# grid_search_nn.fit(X_train, y_train)\n",
    "\n",
    "# # Best Neural Network model\n",
    "# best_nn_model = grid_search_nn.best_estimator_\n",
    "# print(f\"Best NN params: {grid_search_nn.best_params_}\")\n",
    "# print(f\"Best NN accuracy: {grid_search_nn.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86799f68-7db3-4e06-8602-47e178424263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Cross-validate Random Forest base model\n",
    "# rf_model = RandomForestClassifier()\n",
    "\n",
    "# rf_param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [10, 20, None],\n",
    "#     'min_samples_split': [2, 5, 10]\n",
    "# }\n",
    "\n",
    "# grid_search_rf = GridSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_grid=rf_param_grid,\n",
    "#     cv=tscv,\n",
    "#     scoring='accuracy',\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# # Best Random Forest model\n",
    "# best_rf_model = grid_search_rf.best_estimator_\n",
    "# print(f\"Best RF params: {grid_search_rf.best_params_}\")\n",
    "# print(f\"Best RF accuracy: {grid_search_rf.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529d4b9-f4f8-4d35-807a-bf83070ccc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the XGBoost model\n",
    "# xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# # Define the hyperparameters to tune\n",
    "# xgb_param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [3, 6, 9],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'subsample': [0.8, 1.0],\n",
    "#     'colsample_bytree': [0.8, 1.0]\n",
    "# }\n",
    "\n",
    "# # Grid search for hyperparameter tuning\n",
    "# grid_search_xgb = GridSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_grid=xgb_param_grid,\n",
    "#     cv=tscv,\n",
    "#     scoring='accuracy',\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Fit the grid search\n",
    "# grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# # Best XGBoost model\n",
    "# best_xgb_model = grid_search_xgb.best_estimator_\n",
    "# print(f\"Best XGBoost params: {grid_search_xgb.best_params_}\")\n",
    "# print(f\"Best XGBoost accuracy: {grid_search_xgb.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b6fdb-d9cc-4995-9dd6-14561fc43dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create the Keras model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.3, neurons=64, learn_rate=0.01):\n",
    "#     model = Sequential([\n",
    "#         Dense(128, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.001), input_dim=X_train.shape[1]),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(32, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(16, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(5, kernel_initializer='he_uniform', activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, name='adam'), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Example: Cross-validate Neural Network (Keras) base model\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Initialize base models\n",
    "# nn_model = KerasClassifier(build_fn=lambda: create_model(X_train.shape[1]), epochs=100, batch_size=32, verbose=0)\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=5)\n",
    "# xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', colsample_bytree=0.8,\n",
    "#                           learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8)\n",
    "\n",
    "# # Fit the base models\n",
    "# nn_model.fit(X_train, y_train)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# rf_predictions_train = rf_model.predict_proba(X_train)\n",
    "# xgb_predictions_train = xgb_model.predict_proba(X_train)\n",
    "# nn_predictions_train = nn_model.predict_proba(X_train)\n",
    "\n",
    "# meta_train_X = np.hstack([rf_predictions_train, xgb_predictions_train, nn_predictions_train])\n",
    "# meta_train_y = y_train  # Your training labels\n",
    "\n",
    "# meta_model = LogisticRegression()\n",
    "\n",
    "# # Set up the TimeSeriesSplit for time-aware cross-validation\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# # Define the hyperparameters to tune for Logistic Regression\n",
    "# param_grid = {\n",
    "#     'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "#     'solver': ['liblinear', 'lbfgs'],  # Different solvers for logistic regression\n",
    "#     'max_iter': [100, 200]  # Maximum number of iterations\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV to find the best hyperparameters\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=meta_model,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=tscv,\n",
    "#     scoring='accuracy',  # You can change this to 'f1', 'roc_auc', etc., depending on your metric of choice\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1  # Use all available cores for parallel processing\n",
    "# )\n",
    "\n",
    "# # Perform the hyperparameter search using GridSearchCV\n",
    "# grid_search.fit(meta_train_X, meta_train_y)\n",
    "\n",
    "# # Get the best meta-model from the grid search\n",
    "# best_meta_model = grid_search.best_estimator_\n",
    "\n",
    "# # Output the best hyperparameters\n",
    "# print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e29247",
   "metadata": {},
   "source": [
    "### Continue programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b81a41-5d52-4145-aa95-ab706fda0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model function\n",
    "def create_simple_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.001), input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(5, kernel_initializer='he_uniform', activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, name='adam'), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    # Initialize base models\n",
    "    nn_model = KerasClassifier(build_fn=lambda: create_simple_model(X_train.shape[1]), epochs=100, batch_size=32, verbose=0)\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=5)\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', colsample_bytree=0.8,\n",
    "                              learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8)\n",
    "    \n",
    "    # Fit the base models\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for train_index, val_index in tscv.split(X_train):\n",
    "        X_t, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_t, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Re-instantiate KerasClassifier for each fold with proper input_dim\n",
    "        nn_model = KerasClassifier(build_fn=lambda: create_simple_model(X_train.shape[1]), epochs=100, batch_size=32, verbose=0)\n",
    "        \n",
    "        # Fit base models\n",
    "        rf_model.fit(X_t, y_t)\n",
    "        xgb_model.fit(X_t, y_t)\n",
    "        \n",
    "        # Get predictions from the neural network for the validation fold\n",
    "        nn_model.fit(X_t, y_t)  # Fit the neural network on the training fold\n",
    "        nn_predictions_val = nn_model.predict_proba(X_val)  # Get predictions for the validation fold\n",
    "        \n",
    "        # Get predictions from the other base models for the validation fold\n",
    "        rf_predictions_val = rf_model.predict_proba(X_val)\n",
    "        xgb_predictions_val = xgb_model.predict_proba(X_val)\n",
    "        \n",
    "        # Combine predictions from all models for stacking\n",
    "        meta_features_val = np.hstack([rf_predictions_val, xgb_predictions_val, nn_predictions_val])\n",
    "        \n",
    "        # Train a meta-model (LogisticRegression in this case) on the combined predictions\n",
    "        meta_model = LogisticRegression(C=100, max_iter=100, solver='liblinear')\n",
    "        meta_model.fit(meta_features_val, y_val)\n",
    "        \n",
    "        # Evaluate the meta-model on the validation set\n",
    "        meta_predictions_val = meta_model.predict(meta_features_val)\n",
    "        accuracy = accuracy_score(y_val, meta_predictions_val)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    # After cross-validation, calculate the average accuracy\n",
    "    average_accuracy = np.mean(fold_accuracies)\n",
    "\n",
    "    return average_accuracy,rf_model,xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6b49b0-b086-4bb6-ab36-4b72742e1736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def final_models(X,y):\n",
    "    # Combine scaled features for the entire dataset\n",
    "    X_full_scaled = preprocessor.fit_transform(X)\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    \n",
    "    # Train base models on the full dataset\n",
    "    rf_model.fit(X_full_scaled, y_encoded)\n",
    "    xgb_model.fit(X_full_scaled, y_encoded)\n",
    "    nn_model_full = KerasClassifier(build_fn=lambda: create_simple_model(X_full_scaled.shape[1]), \n",
    "                                    epochs=100, batch_size=32, verbose=0)\n",
    "    nn_model_full.fit(X_full_scaled, y_encoded)\n",
    "    \n",
    "    # Generate predictions (probabilities) for stacking\n",
    "    rf_predictions_full = rf_model.predict_proba(X_full_scaled)\n",
    "    xgb_predictions_full = xgb_model.predict_proba(X_full_scaled)\n",
    "    nn_predictions_full = nn_model_full.predict_proba(X_full_scaled)\n",
    "    \n",
    "    # Stack predictions to form meta-features\n",
    "    meta_features_full = np.hstack([rf_predictions_full, xgb_predictions_full, nn_predictions_full])\n",
    "    \n",
    "    # Train meta-model (Logistic Regression) on the full dataset's meta-features\n",
    "    meta_model_full = LogisticRegression()\n",
    "    meta_model_full.fit(meta_features_full, y_encoded)\n",
    "\n",
    "    return nn_model_full,meta_model_full\n",
    "    \n",
    "    print(\"Final ensemble model trained on the full dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8269cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(encoder,preprocessor,average_accuracy,rf_model,xgb_model,nn_model_full,meta_model_full):\n",
    "    with open('encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    with open('preprocessor.pkl', 'wb') as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    with open('accuracy.pkl', 'wb') as f:\n",
    "        pickle.dump(average_accuracy, f)\n",
    "    \n",
    "    # Save RandomForest model\n",
    "    joblib.dump(rf_model, 'rf_model.pkl')\n",
    "    \n",
    "    # Save XGBoost model\n",
    "    joblib.dump(xgb_model, 'xgb_model.pkl')\n",
    "    \n",
    "    # Save the neural network model\n",
    "    save_model(nn_model_full.model, 'nn_model.h5')\n",
    "    \n",
    "    # Save the meta-model (Logistic Regression)\n",
    "    joblib.dump(meta_model_full, 'meta_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873dfce3-cdcb-437c-baa8-fb8664afd355",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: '.\\\\cleaned data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     cleaned_data,website_code\u001b[38;5;241m=\u001b[39mset_wd()\n\u001b[1;32m----> 3\u001b[0m     encoder,preprocessor,X_train,y_train,X,y \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     average_accuracy, rf_model, xgb_model \u001b[38;5;241m=\u001b[39m train_model(X_train, y_train)    \n\u001b[0;32m      5\u001b[0m     nn_model_full, meta_model_full \u001b[38;5;241m=\u001b[39m final_models(X,y)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     match_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafl_match_results_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mchdir(website_code)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: '.\\\\cleaned data'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cleaned_data,website_code=set_wd()\n",
    "    encoder,preprocessor,X_train,y_train,X,y = load_dataset()\n",
    "    average_accuracy, rf_model, xgb_model = train_model(X_train, y_train)    \n",
    "    nn_model_full, meta_model_full = final_models(X,y)\n",
    "    save_models(encoder,preprocessor,average_accuracy,rf_model,xgb_model,nn_model_full,meta_model_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
